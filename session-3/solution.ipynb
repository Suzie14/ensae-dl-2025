{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST with an MLP, from scratch\n",
    "\n",
    "# - Step 1: build an MLP from scratch to solve MNIST. Question set: https://fleuret.org/dlc/materials/dlc-practical-3.pdf\n",
    "# - Step 2: debug your network with backprop ninja and a reference implementation using torch's .backward()\n",
    "# - Step 3: build the same MLP but will full pytorch code (nn.Linear, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Using MNIST\n",
      "** Reduce the data-set (use --full for the full thing)\n",
      "** Use 1000 train and 1000 test samples\n"
     ]
    }
   ],
   "source": [
    "train_input, train_target, test_input, test_target = load_data(one_hot_labels = True, normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 784])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x14b5c2900>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.imshow(train_input[4].view((28,28)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy (preds, targets):\n",
    "    \"\"\" Computes the accuracy between predictions and targets. Data is expected to be one-hot encoded. \"\"\"\n",
    "    _, idx1 = torch.max(preds, dim=1)\n",
    "    _, idx2 = torch.max(targets, dim=1)\n",
    "    d = idx1 == idx2\n",
    "    return d.int().float().mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# unit test\n",
    "preds = torch.zeros((4,7))\n",
    "preds[0,1] = 1\n",
    "preds[1,4] = 1\n",
    "preds[2,2] = 1\n",
    "preds[3,6] = 1\n",
    "targets = torch.zeros((4,7))\n",
    "targets[0,1] = 1\n",
    "targets[1,4] = 1\n",
    "targets[2,2] = 1\n",
    "targets[3,2] = 1\n",
    "compute_accuracy(preds, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigma(x):\n",
    "    return torch.tanh(x)\n",
    "\n",
    "def dsigma(x):\n",
    "    return (1.0 - torch.tanh(x)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss (v,t):\n",
    "    out = torch.sum(torch.pow(v-t, 2))\n",
    "    return out\n",
    "\n",
    "def dloss(v,t):\n",
    "    return -2.0 * (t-v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1337, -1.4099, -0.7194,  0.9427,  5.7936,  2.3108],\n",
       "        [ 1.4946,  1.2805,  0.9905, -1.9410, -1.7132,  1.4164],\n",
       "        [-0.4312,  1.1042,  6.3547,  0.2399,  0.9457, -0.8823]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = torch.randn((3, 6), dtype=torch.float32)\n",
    "t = torch.randn((3, 6), dtype=torch.float32)\n",
    "l=loss(v,t)\n",
    "dloss(v,t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiply targets by 0.9 to be in the range of tanh\n",
    "train_target *= 0.9\n",
    "test_target *= 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Backprop ninja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility function we will use later when comparing manual gradients to PyTorch gradients\n",
    "def cmp(s, dt, t):\n",
    "  ex = torch.all(dt == t.grad).item()\n",
    "  app = torch.allclose(dt, t.grad)\n",
    "  maxdiff = (dt - t.grad).abs().max().item()\n",
    "  print(f'{s:15s} | exact: {str(ex):5s} | approximate: {str(app):5s} | maxdiff: {maxdiff}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "w1 = torch.randn((784, 50))\n",
    "b1 = torch.randn((50))\n",
    "w2 = torch.randn((50, 10))\n",
    "b2 = torch.randn((10))\n",
    "parameters = [w1, b1, w2, b2]\n",
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 10]), tensor(43.2825, grad_fn=<SumBackward0>))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1 = train_input[:5]\n",
    "y1 = train_target[:5]\n",
    "z1 = x1 @ w1 + b1\n",
    "h1 = sigma(z1)\n",
    "z2 = h1 @ w2 + b2\n",
    "h2 = sigma(z2)\n",
    "l = loss(h2, y1)\n",
    "h2.shape, l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss=43.282527923583984\n"
     ]
    }
   ],
   "source": [
    "others = [h2,z2,h1,z1]\n",
    "for p in parameters:\n",
    "    p.grad = None\n",
    "for t in others:\n",
    "    t.retain_grad()\n",
    "l.backward()\n",
    "print(f'loss={l}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b2.grad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 50])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u = torch.randn((50,))\n",
    "z1.shape\n",
    "#(z1 + torch.randn((50,))).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h2              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "z2              | exact: False | approximate: True  | maxdiff: 1.6298145055770874e-09\n",
      "w2              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "b2              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "h1              | exact: False | approximate: True  | maxdiff: 1.4901161193847656e-08\n",
      "z1              | exact: False | approximate: True  | maxdiff: 4.423782229423523e-09\n",
      "w1              | exact: False | approximate: True  | maxdiff: 5.960464477539063e-08\n",
      "b1              | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "# here we compare our gradient to the reference gradient computed by pytorch\n",
    "dl = 1.0\n",
    "dh2 = dloss(h2, y1) * dl\n",
    "dh2.shape\n",
    "cmp('h2',dh2,h2)\n",
    "dz2 = dsigma(z2) * dh2\n",
    "dz2.shape\n",
    "cmp('z2',dz2, z2)\n",
    "dw2 = h1.T @ dz2\n",
    "cmp('w2',dw2, w2)\n",
    "db2 = dz2.sum(0, keepdim=True)\n",
    "#print(b2.shape)\n",
    "cmp('b2',db2, b2)\n",
    "dh1 = dz2 @ w2.T\n",
    "cmp('h1',dh1, h1)\n",
    "dz1 = dsigma(z1) * dh1\n",
    "cmp('z1', dz1, z1)\n",
    "dw1 = x1.T @ dz1\n",
    "cmp('w1', dw1, w1)\n",
    "db1 = dz1.sum(0, keepdim=True)\n",
    "cmp('b1', db1, b1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.1\n",
    "with torch.no_grad():\n",
    "    w1 += -lr * dw1\n",
    "    b1 += -lr * db1.squeeze()\n",
    "    w2 += -lr * dw2\n",
    "    b2 += -lr * db2.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43.282527923583984"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = loss(h2, y1)\n",
    "l.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Now that we've checked our gradients are correct, we can implement the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(w1, b1, w2, b2, x):\n",
    "    z1 = x @ w1 + b1\n",
    "    h1 = sigma(z1)\n",
    "    z2 = h1 @ w2 + b2\n",
    "    h2 = sigma(z2)\n",
    "    return z1, h1, z2, h2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(w1, b1, w2, b2, x1, y1, h2, z2, h1, z1):\n",
    "    dl = 1.0\n",
    "    dh2 = dloss(h2, y1) * dl\n",
    "    dz2 = dsigma(z2) * dh2\n",
    "    dw2 = h1.T @ dz2\n",
    "    db2 = dz2.sum(0, keepdim=True)\n",
    "    dh1 = dz2 @ w2.T\n",
    "    dz1 = dsigma(z1) * dh1\n",
    "    dw1 = x1.T @ dz1\n",
    "    db1 = dz1.sum(0, keepdim=True)\n",
    "    return dw1, db1, dw2, db2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(w1, b1, w2, b2, dw1, db1, dw2, db2, lr):\n",
    "    with torch.no_grad():\n",
    "        w1 += -lr * dw1\n",
    "        b1 += -lr * db1.squeeze()\n",
    "        w2 += -lr * dw2\n",
    "        b2 += -lr * db2.squeeze()\n",
    "    return w1, b1, w2, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init():\n",
    "    \"\"\" init a network \"\"\"\n",
    "    torch.manual_seed(1337)\n",
    "    istd = math.sqrt(1e-6)\n",
    "    w1 = torch.zeros((784, 50))\n",
    "    torch.nn.init.normal_(w1, mean=.0, std=istd)\n",
    "    b1 = torch.zeros((50,))\n",
    "    w2 = torch.zeros((50, 10))\n",
    "    torch.nn.init.normal_(w2, mean=.0, std=istd)\n",
    "    b2 = torch.zeros((10,))\n",
    "    return w1, b1, w2, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1, b1, w2, b2 = init()\n",
    "parameters = [w1, b1, w2, b2]\n",
    "for p in parameters:\n",
    "    p.requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 784])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main training loop\n",
    "torch.set_printoptions(linewidth=200)\n",
    "def train(w1, b1, w2, b2):\n",
    "    lossi = []\n",
    "    for step in range(10000):\n",
    "        xb = train_input\n",
    "        yb = train_target\n",
    "        num_samples = xb.shape[0]\n",
    "        #print(f'{num_samples=}')\n",
    "        # forward\n",
    "        z1, h1, z2, h2 = forward(w1, b1, w2, b2, xb)\n",
    "        lsi = loss(h2, yb)\n",
    "        # backward\n",
    "        dw1, db1, dw2, db2 = backward(w1, b1, w2, b2, xb, yb, h2, z2, h1, z1)\n",
    "        # update\n",
    "        lr = 0.1 / num_samples if step < 5000 else 0.01 / num_samples\n",
    "        w1, b1, w2, b2 = update(w1, b1, w2, b2, dw1, db1, dw2, db2, lr)\n",
    "        if step % 100 == 0: print(f'step = {step}, loss = {lsi}')\n",
    "        lossi.append(lsi.item())\n",
    "    # compute accuracy\n",
    "    _, _, _, preds = forward(w1, b1, w2, b2, train_input)\n",
    "    train_accuracy = compute_accuracy(preds, train_target)\n",
    "    _, _, _, preds = forward(w1, b1, w2, b2, test_input)\n",
    "    test_accuracy = compute_accuracy(preds, test_target)\n",
    "    print(f'{train_accuracy=}')\n",
    "    print(f'{test_accuracy=}')\n",
    "    return lossi\n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 0, loss = 809.9498291015625\n",
      "step = 100, loss = 288.54022216796875\n",
      "step = 200, loss = 195.53509521484375\n",
      "step = 300, loss = 139.04873657226562\n",
      "step = 400, loss = 135.3380126953125\n",
      "step = 500, loss = 99.42034149169922\n",
      "step = 600, loss = 82.91433715820312\n",
      "step = 700, loss = 72.19515991210938\n",
      "step = 800, loss = 62.638893127441406\n",
      "step = 900, loss = 61.924476623535156\n",
      "step = 1000, loss = 58.28996276855469\n",
      "step = 1100, loss = 51.019866943359375\n",
      "step = 1200, loss = 41.54011535644531\n",
      "step = 1300, loss = 55.92261505126953\n",
      "step = 1400, loss = 31.951839447021484\n",
      "step = 1500, loss = 37.646663665771484\n",
      "step = 1600, loss = 33.600311279296875\n",
      "step = 1700, loss = 29.174652099609375\n",
      "step = 1800, loss = 24.88680648803711\n",
      "step = 1900, loss = 29.827388763427734\n",
      "step = 2000, loss = 27.54845428466797\n",
      "step = 2100, loss = 27.90268325805664\n",
      "step = 2200, loss = 24.179628372192383\n",
      "step = 2300, loss = 24.80556869506836\n",
      "step = 2400, loss = 21.696060180664062\n",
      "step = 2500, loss = 18.241796493530273\n",
      "step = 2600, loss = 17.214893341064453\n",
      "step = 2700, loss = 23.833635330200195\n",
      "step = 2800, loss = 21.587316513061523\n",
      "step = 2900, loss = 19.335237503051758\n",
      "step = 3000, loss = 18.752338409423828\n",
      "step = 3100, loss = 17.32056427001953\n",
      "step = 3200, loss = 20.57852554321289\n",
      "step = 3300, loss = 18.7183837890625\n",
      "step = 3400, loss = 14.38791275024414\n",
      "step = 3500, loss = 16.53356170654297\n",
      "step = 3600, loss = 18.521244049072266\n",
      "step = 3700, loss = 15.777360916137695\n",
      "step = 3800, loss = 14.918214797973633\n",
      "step = 3900, loss = 14.013114929199219\n",
      "step = 4000, loss = 13.87615966796875\n",
      "step = 4100, loss = 12.341063499450684\n",
      "step = 4200, loss = 16.057085037231445\n",
      "step = 4300, loss = 15.566722869873047\n",
      "step = 4400, loss = 11.359606742858887\n",
      "step = 4500, loss = 14.306371688842773\n",
      "step = 4600, loss = 12.054241180419922\n",
      "step = 4700, loss = 12.753377914428711\n",
      "step = 4800, loss = 10.485122680664062\n",
      "step = 4900, loss = 10.97787857055664\n",
      "step = 5000, loss = 16.134607315063477\n",
      "step = 5100, loss = 5.329019546508789\n",
      "step = 5200, loss = 5.286474704742432\n",
      "step = 5300, loss = 5.251482963562012\n",
      "step = 5400, loss = 5.220162391662598\n",
      "step = 5500, loss = 5.191140651702881\n",
      "step = 5600, loss = 5.163758754730225\n",
      "step = 5700, loss = 5.137642860412598\n",
      "step = 5800, loss = 5.112558364868164\n",
      "step = 5900, loss = 5.08833646774292\n",
      "step = 6000, loss = 5.064859390258789\n",
      "step = 6100, loss = 5.042037487030029\n",
      "step = 6200, loss = 5.0197954177856445\n",
      "step = 6300, loss = 4.998077392578125\n",
      "step = 6400, loss = 4.976835250854492\n",
      "step = 6500, loss = 4.956025123596191\n",
      "step = 6600, loss = 4.935614109039307\n",
      "step = 6700, loss = 4.9155731201171875\n",
      "step = 6800, loss = 4.895873069763184\n",
      "step = 6900, loss = 4.876492977142334\n",
      "step = 7000, loss = 4.8574113845825195\n",
      "step = 7100, loss = 4.838611602783203\n",
      "step = 7200, loss = 4.820075035095215\n",
      "step = 7300, loss = 4.801788330078125\n",
      "step = 7400, loss = 4.78373908996582\n",
      "step = 7500, loss = 4.765915870666504\n",
      "step = 7600, loss = 4.748308181762695\n",
      "step = 7700, loss = 4.730906009674072\n",
      "step = 7800, loss = 4.713698387145996\n",
      "step = 7900, loss = 4.696678161621094\n",
      "step = 8000, loss = 4.679839134216309\n",
      "step = 8100, loss = 4.663171291351318\n",
      "step = 8200, loss = 4.646668434143066\n",
      "step = 8300, loss = 4.63032865524292\n",
      "step = 8400, loss = 4.614144325256348\n",
      "step = 8500, loss = 4.598108768463135\n",
      "step = 8600, loss = 4.582220077514648\n",
      "step = 8700, loss = 4.566468238830566\n",
      "step = 8800, loss = 4.5508527755737305\n",
      "step = 8900, loss = 4.53537130355835\n",
      "step = 9000, loss = 4.520016670227051\n",
      "step = 9100, loss = 4.504786968231201\n",
      "step = 9200, loss = 4.489679336547852\n",
      "step = 9300, loss = 4.4746904373168945\n",
      "step = 9400, loss = 4.459817886352539\n",
      "step = 9500, loss = 4.4450554847717285\n",
      "step = 9600, loss = 4.430405616760254\n",
      "step = 9700, loss = 4.415860652923584\n",
      "step = 9800, loss = 4.401421546936035\n",
      "step = 9900, loss = 4.387086391448975\n",
      "train_accuracy=1.0\n",
      "test_accuracy=0.8190000057220459\n"
     ]
    }
   ],
   "source": [
    "lossi = train(w1, b1, w2, b2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x169cad970>]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.plot(lossi)\n",
    "#print(lossi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Reference implementation using pytorch's .backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1, b1, w2, b2 = init()\n",
    "parameters = [w1, b1, w2, b2]\n",
    "for p in parameters:\n",
    "    p.requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference code\n",
    "torch.set_printoptions(linewidth=200)\n",
    "import torch.nn as F\n",
    "\n",
    "def train(w1, b1, w2, b2):\n",
    "    lossi = []\n",
    "    for step in range(10000):\n",
    "        xb = train_input\n",
    "        yb = train_target\n",
    "        num_samples = xb.shape[0]\n",
    "        # forward\n",
    "        z1, h1, z2, h2 = forward(w1, b1, w2, b2, xb)\n",
    "        xloss = F.MSELoss()\n",
    "        lsi = xloss(h2, yb) * yb.nelement()\n",
    "        # backward\n",
    "        for p in parameters:\n",
    "            p.grad = None\n",
    "        lsi.backward()\n",
    "        # update\n",
    "        lr = 0.1 / num_samples\n",
    "        for p in parameters:\n",
    "            p.data += -lr * p.grad\n",
    "        if step % 100 == 0: print(f'step = {step}, loss = {lsi}')\n",
    "        lossi.append(lsi.item())\n",
    "    # compute accuracy\n",
    "    _, _, _, preds = forward(w1, b1, w2, b2, train_input)\n",
    "    train_accuracy = compute_accuracy(preds, train_target)\n",
    "    _, _, _, preds = forward(w1, b1, w2, b2, test_input)\n",
    "    test_accuracy = compute_accuracy(preds, test_target)\n",
    "    print(f'{train_accuracy=}')\n",
    "    print(f'{test_accuracy=}')\n",
    "    return lossi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 0, loss = 809.9498901367188\n",
      "step = 100, loss = 288.32464599609375\n",
      "step = 200, loss = 181.5811767578125\n",
      "step = 300, loss = 145.126220703125\n",
      "step = 400, loss = 126.37185668945312\n",
      "step = 500, loss = 95.3000717163086\n",
      "step = 600, loss = 79.75885772705078\n",
      "step = 700, loss = 72.97013092041016\n",
      "step = 800, loss = 60.800933837890625\n",
      "step = 900, loss = 67.0023422241211\n",
      "step = 1000, loss = 54.57250213623047\n",
      "step = 1100, loss = 67.70767974853516\n",
      "step = 1200, loss = 47.989532470703125\n",
      "step = 1300, loss = 42.321044921875\n",
      "step = 1400, loss = 37.51057815551758\n",
      "step = 1500, loss = 47.89775085449219\n",
      "step = 1600, loss = 40.352420806884766\n",
      "step = 1700, loss = 34.35886764526367\n",
      "step = 1800, loss = 32.882774353027344\n",
      "step = 1900, loss = 25.800189971923828\n",
      "step = 2000, loss = 27.818504333496094\n",
      "step = 2100, loss = 24.97629737854004\n",
      "step = 2200, loss = 34.46820068359375\n",
      "step = 2300, loss = 24.227466583251953\n",
      "step = 2400, loss = 25.058338165283203\n",
      "step = 2500, loss = 20.51760482788086\n",
      "step = 2600, loss = 22.06326675415039\n",
      "step = 2700, loss = 16.642478942871094\n",
      "step = 2800, loss = 18.774250030517578\n",
      "step = 2900, loss = 19.451576232910156\n",
      "step = 3000, loss = 18.840866088867188\n",
      "step = 3100, loss = 14.671106338500977\n",
      "step = 3200, loss = 16.341527938842773\n",
      "step = 3300, loss = 17.477954864501953\n",
      "step = 3400, loss = 15.570767402648926\n",
      "step = 3500, loss = 15.023197174072266\n",
      "step = 3600, loss = 14.618420600891113\n",
      "step = 3700, loss = 14.586233139038086\n",
      "step = 3800, loss = 11.909760475158691\n",
      "step = 3900, loss = 14.052265167236328\n",
      "step = 4000, loss = 13.978957176208496\n",
      "step = 4100, loss = 18.507427215576172\n",
      "step = 4200, loss = 14.809917449951172\n",
      "step = 4300, loss = 13.675326347351074\n",
      "step = 4400, loss = 11.791366577148438\n",
      "step = 4500, loss = 11.610532760620117\n",
      "step = 4600, loss = 14.234821319580078\n",
      "step = 4700, loss = 10.731109619140625\n",
      "step = 4800, loss = 10.805072784423828\n",
      "step = 4900, loss = 8.856196403503418\n",
      "step = 5000, loss = 9.38582992553711\n",
      "step = 5100, loss = 11.829460144042969\n",
      "step = 5200, loss = 9.222826957702637\n",
      "step = 5300, loss = 9.922097206115723\n",
      "step = 5400, loss = 11.078106880187988\n",
      "step = 5500, loss = 11.599679946899414\n",
      "step = 5600, loss = 8.264585494995117\n",
      "step = 5700, loss = 7.832922458648682\n",
      "step = 5800, loss = 11.632868766784668\n",
      "step = 5900, loss = 6.224124431610107\n",
      "step = 6000, loss = 7.852828025817871\n",
      "step = 6100, loss = 8.72516918182373\n",
      "step = 6200, loss = 7.659221649169922\n",
      "step = 6300, loss = 6.364897727966309\n",
      "step = 6400, loss = 9.708893775939941\n",
      "step = 6500, loss = 8.532472610473633\n",
      "step = 6600, loss = 5.542931079864502\n",
      "step = 6700, loss = 7.150405406951904\n",
      "step = 6800, loss = 8.234230041503906\n",
      "step = 6900, loss = 7.140960216522217\n",
      "step = 7000, loss = 5.4433207511901855\n",
      "step = 7100, loss = 8.884539604187012\n",
      "step = 7200, loss = 6.177454471588135\n",
      "step = 7300, loss = 5.433903217315674\n",
      "step = 7400, loss = 8.791483879089355\n",
      "step = 7500, loss = 7.449447154998779\n",
      "step = 7600, loss = 5.355076313018799\n",
      "step = 7700, loss = 7.035911560058594\n",
      "step = 7800, loss = 5.96937370300293\n",
      "step = 7900, loss = 7.098382949829102\n",
      "step = 8000, loss = 5.83750581741333\n",
      "step = 8100, loss = 6.7147321701049805\n",
      "step = 8200, loss = 7.4992780685424805\n",
      "step = 8300, loss = 5.730503559112549\n",
      "step = 8400, loss = 6.861331462860107\n",
      "step = 8500, loss = 5.815117835998535\n",
      "step = 8600, loss = 6.279022693634033\n",
      "step = 8700, loss = 4.437047481536865\n",
      "step = 8800, loss = 4.445477485656738\n",
      "step = 8900, loss = 4.606042385101318\n",
      "step = 9000, loss = 6.1560959815979\n",
      "step = 9100, loss = 5.169589996337891\n",
      "step = 9200, loss = 4.7121100425720215\n",
      "step = 9300, loss = 5.239711761474609\n",
      "step = 9400, loss = 4.545611381530762\n",
      "step = 9500, loss = 5.853402614593506\n",
      "step = 9600, loss = 6.297412872314453\n",
      "step = 9700, loss = 5.937180042266846\n",
      "step = 9800, loss = 5.874582767486572\n",
      "step = 9900, loss = 4.782405853271484\n",
      "train_accuracy=1.0\n",
      "test_accuracy=0.8169999718666077\n"
     ]
    }
   ],
   "source": [
    "lossi = train(w1, b1, w2, b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.plot(lossi)\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Build the same MLP layer but with fully pytorch code (nn.Linear(), etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# network dimensions\n",
    "n_in = 784\n",
    "n_hidden = 200\n",
    "n_out = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr, Y_tr = train_input, train_target\n",
    "X_test, Y_test = test_input, test_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList((\n",
    "            nn.Linear(n_in, n_hidden, bias=True),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(n_hidden, n_out, bias=True), \n",
    "            nn.Tanh(), \n",
    "        ))\n",
    "\n",
    "    def __call__(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "    \n",
    "    def __parameters__(self):\n",
    "        return [p for layer in self.layers for p in layer.parameters]\n",
    "\n",
    "model = MLP()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "loss_fn = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step =      0\tloss=0.14046\taccuracy (train, test): 0.11500\t0.19800\n",
      "step =   1000\tloss=0.00009\taccuracy (train, test): 1.00000\t0.87300\n",
      "step =   2000\tloss=0.00005\taccuracy (train, test): 1.00000\t0.87500\n",
      "step =   3000\tloss=0.00009\taccuracy (train, test): 1.00000\t0.87300\n",
      "step =   4000\tloss=0.00002\taccuracy (train, test): 1.00000\t0.87700\n",
      "step =   5000\tloss=0.00003\taccuracy (train, test): 1.00000\t0.87800\n",
      "step =   6000\tloss=0.00012\taccuracy (train, test): 1.00000\t0.88100\n",
      "step =   7000\tloss=0.00000\taccuracy (train, test): 1.00000\t0.88000\n",
      "step =   8000\tloss=0.00004\taccuracy (train, test): 1.00000\t0.88100\n",
      "step =   9000\tloss=0.00003\taccuracy (train, test): 1.00000\t0.88200\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "num_epochs = 10000\n",
    "\n",
    "for n in range(num_epochs):\n",
    "    y_pred = model(X_tr)\n",
    "    loss = loss_fn(y_pred, Y_tr)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if n % 1000 == 0: \n",
    "        with torch.no_grad():\n",
    "            # train accuracy\n",
    "            acc_train = compute_accuracy(y_pred, Y_tr)\n",
    "            # test accuracy\n",
    "            y_test_preds = model(X_test)\n",
    "            acc_test = compute_accuracy(y_test_preds, Y_test)\n",
    "            print(f'step = {n:6d}\\tloss={loss.item():.5f}\\taccuracy (train, test): {acc_train:.5f}\\t{acc_test:.5f}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise: try to improve accuracy!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
