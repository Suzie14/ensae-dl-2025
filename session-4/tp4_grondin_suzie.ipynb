{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Self-attention by hand\n",
    "# 2. Self-attention block in pytorch\n",
    "# 3. GPT, piece-by-piece\n",
    "# 4. GPU goes rrrr!\n",
    "\n",
    "# Original code from https://github.com/karpathy/minGPT/tree/master/mingpt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Self-attention by hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  -- Write the scaled dot product self attention\n",
    "  # 1. Compute queries, keys, and values\n",
    "  # 2. Compute dot products\n",
    "  # 3. Scale the dot products\n",
    "  # 4. Apply softmax to calculate attentions\n",
    "  # 5. Weight values by attentions\n",
    "  # 6. Compute attention weighted features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose random values for the parameters -- sames values as on slide 12, but in pytorch format\n",
    "# T = 4, C = 6, H = 3\n",
    "X = torch.tensor([[2,0,0,0,2,1],[0,1,2,0,0,0],[0,0,1,1,0,1],[2,0,0,1,0,1]], dtype=float) # T x C\n",
    "W_QT = torch.tensor([[1,0,0], [1,1,0], [0,0,1], [0,1,0], [0,0,1], [0,0,1]], dtype=float) # C x H\n",
    "W_KT = torch.tensor([[0,0,1], [0,1,0], [1,0,0], [0,0,0], [0,0,0], [0,0,-1]], dtype=float) # C x H\n",
    "W_VT = torch.tensor([[10,0,0], [0,0,10], [0,0,0], [0,10,0], [0,0,0], [0,0,0]], dtype=float) # C x H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = X @ W_QT\n",
    "K = X @ W_KT\n",
    "V = X @ W_VT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What does the second dimension of matrices Q and K correspond to?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It correspond to the head size "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the weighted attention matrix S\n",
    "S = Q @ K.T\n",
    "d_k = Q.shape[1]\n",
    "S /= math.sqrt(d_k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the self-attention matrix A\n",
    "A = F.softmax(S, dim=-1) @ V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sanity check. This should return True.\n",
    "torch.allclose(A.float(), torch.tensor([[10.30759701,  2.83283874,  4.59026201],\n",
    "        [10.10551833,  2.97334971,  4.50027071],\n",
    "        [15.03361159,  4.13169018,  2.10990693],\n",
    "        [ 3.06082018,  1.53041009,  7.70438486]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Self-attention block in pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.functional import F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not modify this code\n",
    "\n",
    "batch_size = 3 # B\n",
    "block_size = 2 # T\n",
    "n_embd = 3     # C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_printoptions(precision=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a scaled self-attention head without masked attention and without dropout (i.e. just key, query and values)\n",
    "# A matrix multiplication is implemented using the nn.Linear() operator with no bias.\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.values = nn.Linear(n_embd, head_size, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape \n",
    "\n",
    "        key = self.key(x)  \n",
    "        query = self.query(x)  \n",
    "        values = self.values(x)  \n",
    "\n",
    "        attention_scores = (query @ key.transpose(-2, -1)) / math.sqrt(C)  # (B, T, T)\n",
    "        attention_weights = F.softmax(attention_scores, dim=-1)  # (B, T, T)\n",
    "        out = attention_weights @ values  # (B, T, head_size)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.46653441,  0.03306435],\n",
       "         [-0.47224301,  0.04610375]],\n",
       "\n",
       "        [[-0.38105935,  0.02397408],\n",
       "         [-0.39453450,  0.02482043]],\n",
       "\n",
       "        [[-0.29578221,  0.12158969],\n",
       "         [-0.30042297,  0.12526262]]], grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Unit test. Do not modify this code\n",
    "torch.manual_seed(123) # do not remove this line\n",
    "h = Head(2)\n",
    "torch.manual_seed(123) # do not remove this line\n",
    "x = torch.rand((batch_size, block_size, n_embd))\n",
    "out = h(x)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sanity check. This should return True.\n",
    "torch.allclose(out, torch.tensor([[[-0.46728206,  0.03477207],\n",
    "         [-0.47425330,  0.05069541]],\n",
    "        [[-0.38198256,  0.02403205],\n",
    "         [-0.39846635,  0.02506737]],\n",
    "        [[-0.29631630,  0.12201238],\n",
    "         [-0.30199534,  0.12650707]]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 7.47650862e-04, -1.70771778e-03],\n",
       "         [ 2.01028585e-03, -4.59166244e-03]],\n",
       "\n",
       "        [[ 9.23216343e-04, -5.79748303e-05],\n",
       "         [ 3.93185019e-03, -2.46938318e-04]],\n",
       "\n",
       "        [[ 5.34087420e-04, -4.22686338e-04],\n",
       "         [ 1.57237053e-03, -1.24445558e-03]]], grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out - torch.tensor([[[-0.46728206,  0.03477207],\n",
    "         [-0.47425330,  0.05069541]],\n",
    "        [[-0.38198256,  0.02403205],\n",
    "         [-0.39846635,  0.02506737]],\n",
    "        [[-0.29631630,  0.12201238],\n",
    "         [-0.30199534,  0.12650707]]]) # values are closed even if the sanity check did not pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add weighted masked attention and dropout. Dropout comes after the softmax and before the multiplication with the value matrix.\n",
    "# Copy the Head class from the previous exercise and expand upon it.\n",
    "\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size, dropout=0.01):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.values = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)  \n",
    "        \n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size))) \n",
    "\n",
    "    def forward(self, x, mask=True):\n",
    "        B, T, C = x.shape  \n",
    "\n",
    "        key = self.key(x)  \n",
    "        query = self.query(x)  \n",
    "        values = self.values(x)  \n",
    "\n",
    "        attention_scores = (query @ key.transpose(-2, -1)) / math.sqrt(query.shape[-1])  # (B, T, T)\n",
    "\n",
    "        if mask:\n",
    "            attention_scores = attention_scores.masked_fill(self.tril[:T, :T] == 0, float('-inf'))  \n",
    "\n",
    "        attention_weights = F.softmax(attention_scores, dim=-1)  # (B, T, T)\n",
    "        attention_weights = self.dropout(attention_weights)  \n",
    "        out = attention_weights @ values  # (B, T, head_size)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.38323045, -0.16764536],\n",
       "         [-0.47904372,  0.05120752]],\n",
       "\n",
       "        [[-0.14327437,  0.00903951],\n",
       "         [-0.40249124,  0.02532059]],\n",
       "\n",
       "        [[-0.17476673,  0.02467545],\n",
       "         [-0.30504578,  0.12778492]]], grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Unit test. Do not modify this code\n",
    "torch.manual_seed(123) # do not remove this line\n",
    "h = Head(2)\n",
    "torch.manual_seed(123) # do not remove this line\n",
    "x = torch.rand((batch_size, block_size, n_embd))\n",
    "out = h(x)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sanity check. This should return True.\n",
    "torch.allclose(out, torch.tensor([[[-0.37939820, -0.16596894],\n",
    "         [-0.47425330,  0.05069541]],\n",
    "        [[-0.14184165,  0.00894911],\n",
    "         [-0.39846635,  0.02506737]],\n",
    "        [[-0.17301908,  0.02442869],\n",
    "         [-0.30199534,  0.12650707]]])) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-3.83225083e-03, -1.67642534e-03],\n",
       "         [-4.79042530e-03,  5.12104481e-04]],\n",
       "\n",
       "        [[-1.43271685e-03,  9.04006884e-05],\n",
       "         [-4.02489305e-03,  2.53221020e-04]],\n",
       "\n",
       "        [[-1.74765289e-03,  2.46765092e-04],\n",
       "         [-3.05044651e-03,  1.27784908e-03]]], grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out - torch.tensor([[[-0.37939820, -0.16596894],\n",
    "         [-0.47425330,  0.05069541]],\n",
    "        [[-0.14184165,  0.00894911],\n",
    "         [-0.39846635,  0.02506737]],\n",
    "        [[-0.17301908,  0.02442869],\n",
    "         [-0.30199534,  0.12650707]]]) # values are closed even if the sanity check did not pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A multi-head attention module contains a list of heads and a linear projection layer.\n",
    "# The heads are applied to the input and then concatenated along the last dimension, then\n",
    "# the linear layer is applied. Look at the unit test below to determine the dimensions of\n",
    "# the linear layer.\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, head_size, dropout=0.01):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.head_size = head_size\n",
    "\n",
    "        self.heads = nn.ModuleList([Head(head_size, dropout) for _ in range(num_heads)])\n",
    "\n",
    "        self.proj = nn.Linear(num_heads * head_size, n_embd)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([head(x) for head in self.heads], dim=-1)  # (B, T, num_heads * head_size)\n",
    "\n",
    "        out = self.proj(out)  # (B, T, n_embd)\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not modify\n",
    "num_heads = 3\n",
    "head_size = 2\n",
    "n_embd = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unit test. Do not modify this code\n",
    "torch.manual_seed(123) # do not remove this line\n",
    "sa = MultiHeadAttention(num_heads=3, head_size=head_size)\n",
    "torch.manual_seed(123) # do not remove this line\n",
    "x = torch.rand((batch_size, block_size, n_embd))\n",
    "out = sa(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sanity check. This should return True.\n",
    "torch.allclose(out, torch.tensor([[[-0.03730504, -0.07006130, -0.27096999,  0.13144857, -0.45049590,\n",
    "          -0.33217290],\n",
    "         [-0.06818272, -0.04490501, -0.34806073,  0.15622401, -0.45459983,\n",
    "          -0.33084857]],\n",
    "        [[-0.08914752, -0.03846309, -0.36569631,  0.09802882, -0.39963537,\n",
    "          -0.29225215],\n",
    "         [-0.04541985,  0.01269679, -0.25225419,  0.08241771, -0.41533324,\n",
    "          -0.30674040]],\n",
    "        [[ 0.15234883, -0.08591781, -0.10099770,  0.19886394, -0.49236685,\n",
    "          -0.43605998],\n",
    "         [ 0.15418015, -0.01837257, -0.00573672,  0.14228639, -0.48172480,\n",
    "          -0.40757987]]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.00138838,  0.00042240, -0.00295746,  0.00235054, -0.00605676,\n",
       "          -0.00361356],\n",
       "         [-0.00201530,  0.04490501, -0.00452268,  0.00285356, -0.00614014,\n",
       "          -0.00358665]],\n",
       "\n",
       "        [[-0.00244097,  0.00106399, -0.00488073,  0.00167199, -0.00502414,\n",
       "          -0.00280303],\n",
       "         [-0.00155315,  0.00210274, -0.00257748,  0.00135501, -0.00534290,\n",
       "          -0.00309718]],\n",
       "\n",
       "        [[ 0.00246237,  0.00010044,  0.00049363,  0.00371936, -0.00690696,\n",
       "          -0.00572288],\n",
       "         [ 0.00249955,  0.00147188,  0.00242780,  0.00257060, -0.00669086,\n",
       "          -0.00514466]]], grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out - torch.tensor([[[-0.03730504, -0.07006130, -0.27096999,  0.13144857, -0.45049590,\n",
    "          -0.33217290],\n",
    "         [-0.06818272, -0.04490501, -0.34806073,  0.15622401, -0.45459983,\n",
    "          -0.33084857]],\n",
    "        [[-0.08914752, -0.03846309, -0.36569631,  0.09802882, -0.39963537,\n",
    "          -0.29225215],\n",
    "         [-0.04541985,  0.01269679, -0.25225419,  0.08241771, -0.41533324,\n",
    "          -0.30674040]],\n",
    "        [[ 0.15234883, -0.08591781, -0.10099770,  0.19886394, -0.49236685,\n",
    "          -0.43605998],\n",
    "         [ 0.15418015, -0.01837257, -0.00573672,  0.14228639, -0.48172480,\n",
    "          -0.40757987]]]) # values are closed even if the sanity check did not pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a classical feedforward module: linear -> ReLU -> linear\n",
    "# The hidden dimension is four times bigger than the input dimension (see Section 3.3 of Attention is All You Need)\n",
    "#\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(n_embd, 4 * n_embd)  \n",
    "        self.fc2 = nn.Linear(4 * n_embd, n_embd)  \n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)  \n",
    "        out = self.relu(out) \n",
    "        out = self.fc2(out)  \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.58034140,  0.04641047, -0.10707692,  0.21581650, -0.30361831,\n",
       "         -0.07352637],\n",
       "        [-0.48917407,  0.07879593, -0.15972012,  0.17862341, -0.37070656,\n",
       "         -0.07852859],\n",
       "        [-0.48530391,  0.09604470, -0.06524835,  0.16611032, -0.35499069,\n",
       "         -0.08964306]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Unit test. Do not modify this code\n",
    "torch.manual_seed(123) # do not remove this line\n",
    "ff = FeedForward(n_embd)\n",
    "torch.manual_seed(123) # do not remove this line\n",
    "x = torch.rand((3,n_embd))\n",
    "out = ff(x)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sanity check. This should return True.\n",
    "torch.allclose(out, torch.tensor([[-0.58034140,  0.04641046, -0.10707694,  0.21581653, -0.30361831,\n",
    "         -0.07352637],\n",
    "        [-0.48917407,  0.07879593, -0.15972012,  0.17862344, -0.37070659,\n",
    "         -0.07852858],\n",
    "        [-0.48530388,  0.09604470, -0.06524836,  0.16611034, -0.35499069,\n",
    "         -0.08964306]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a self-attention block\n",
    "#\n",
    "#   in -----> LayerNorm -------> multi-head attention -- + ----> LayerNorm -----> FeedForward --- + -----> out\n",
    "#         |                                              |   |                                    |\n",
    "#          ----------------------------------------------     ------------------------------------                       \n",
    "#\n",
    "# This architecture is slightly different from Attention is All You Need (or the UDL textbook):\n",
    "# the layer norm comes before (not after) the attention or feed-forward\n",
    "#\n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.ln1 = nn.LayerNorm(n_embd)  \n",
    "        self.attn = MultiHeadAttention(n_head, head_size)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)  \n",
    "        self.ffn = FeedForward(n_embd)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x + self.attn(self.ln1(x))\n",
    "        out = out + self.ffn(self.ln2(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.05804071, -0.11310092, -0.10850900,  0.98114121, -0.55709964,\n",
       "           0.56820649],\n",
       "         [-0.21670339, -0.27101421, -0.29645509,  1.13258827, -0.34745026,\n",
       "           0.38123190]],\n",
       "\n",
       "        [[-0.42635530, -0.29880306, -0.13436174,  0.65017426, -0.51931226,\n",
       "           0.56973475],\n",
       "         [-0.03636354,  0.09184688,  0.64883506,  0.70156789,  0.05891362,\n",
       "           0.69799930]],\n",
       "\n",
       "        [[ 0.53390098,  0.33858770,  0.30727547,  1.12015176,  0.37155873,\n",
       "          -0.03651971],\n",
       "         [ 1.39353633,  0.59874254,  0.99845648,  0.38251585,  0.61530453,\n",
       "           0.47056967]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Unit test. Do not modify this code\n",
    "torch.manual_seed(123) # do not remove this line\n",
    "bk = Block(n_embd, num_heads)\n",
    "torch.manual_seed(123) # do not remove this line\n",
    "x = torch.rand((batch_size,block_size,n_embd))\n",
    "out = bk(x)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sanity check. This should return True.\n",
    "torch.allclose(out, torch.tensor([[[-0.05278997, -0.10863629, -0.09458938,  0.97590691, -0.55101192,\n",
    "           0.57085067],\n",
    "         [-0.16924502, -0.45394337, -0.25217158,  1.10904062, -0.34593600,\n",
    "           0.41432184]],\n",
    "        [[-0.41515028, -0.30126408, -0.11399293,  0.64651299, -0.51579159,\n",
    "           0.57017863],\n",
    "         [-0.02535054,  0.08704096,  0.66524690,  0.69768047,  0.05969021,\n",
    "           0.69993609]],\n",
    "        [[ 0.52881187,  0.34458166,  0.31130391,  1.11564195,  0.37998506,\n",
    "          -0.02971917],\n",
    "         [ 1.38496208,  0.60325992,  0.99346304,  0.38082033,  0.62151432,\n",
    "           0.47973478]]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.00525074, -0.00446463, -0.01391962,  0.00523430, -0.00608772,\n",
       "          -0.00264418],\n",
       "         [-0.04745837,  0.18292916, -0.04428351,  0.02354765, -0.00151426,\n",
       "          -0.03308994]],\n",
       "\n",
       "        [[-0.01120502,  0.00246102, -0.02036881,  0.00366127, -0.00352067,\n",
       "          -0.00044388],\n",
       "         [-0.01101300,  0.00480592, -0.01641184,  0.00388741, -0.00077659,\n",
       "          -0.00193679]],\n",
       "\n",
       "        [[ 0.00508910, -0.00599396, -0.00402844,  0.00450981, -0.00842634,\n",
       "          -0.00680054],\n",
       "         [ 0.00857425, -0.00451738,  0.00499344,  0.00169551, -0.00620979,\n",
       "          -0.00916511]]], grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out - torch.tensor([[[-0.05278997, -0.10863629, -0.09458938,  0.97590691, -0.55101192,\n",
    "           0.57085067],\n",
    "         [-0.16924502, -0.45394337, -0.25217158,  1.10904062, -0.34593600,\n",
    "           0.41432184]],\n",
    "        [[-0.41515028, -0.30126408, -0.11399293,  0.64651299, -0.51579159,\n",
    "           0.57017863],\n",
    "         [-0.02535054,  0.08704096,  0.66524690,  0.69768047,  0.05969021,\n",
    "           0.69993609]],\n",
    "        [[ 0.52881187,  0.34458166,  0.31130391,  1.11564195,  0.37998506,\n",
    "          -0.02971917],\n",
    "         [ 1.38496208,  0.60325992,  0.99346304,  0.38082033,  0.62151432,\n",
    "           0.47973478]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 3: Build a mini GPT\n",
    "#\n",
    "# - Start from the gpt-problem.py file\n",
    "# - Add your Head, MultiHeadAttention, FeedForward and Block classes ok \n",
    "# - Fill in the GPT class (__init__ and forward methods)\n",
    "# - Train the network on CPU\n",
    "# - Train the network on GPU\n",
    "\n",
    "# For __init__, the GPT model parameters are:\n",
    "#   - a token embedding table\n",
    "#   - a positional embedding table\n",
    "#   - a sequence of Blocks\n",
    "#   - a layer norm\n",
    "#   - a linear layer\n",
    "#\n",
    "# For forward(), the model consists in:\n",
    "#   - applying the token embedding table and positional embedding table to the input tensor\n",
    "#   - adding the two together\n",
    "#   - applying the blocks, layer norm and linear layer (in that order)\n",
    "#\n",
    "# The code comes from hyperparameters that should work well on GPU.  On CPU, you \n",
    "# will need to reduce the model size significantly.\n",
    "#\n",
    "# In pytorch, an learnable embedding table is implemented with nn.Embedding(...)\n",
    "#\n",
    "# The token embedding table learns an embedding for each item of the vocabulary. The \n",
    "# positional embedding table does not depend on the input and learns an embedding\n",
    "# for each position in the context."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
